{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read img files into a list\n",
    "# data_dir = './test_data/Mask/'\n",
    "# # os.listdir(data_dir)\n",
    "\n",
    "# filenames = [name for name in os.listdir(data_dir)\n",
    "#             if os.path.splitext(name)[-1] == '.jpg']\n",
    "\n",
    "# img_arr = imageio.imread(os.path.join(data_dir, filenames[0]))\n",
    "# img_arr.shape\n",
    "\n",
    "# batch_size = 5 # number of imgs in a batch\n",
    "# batch = torch.zeros(batch_size, 3, 256, 256)\n",
    "\n",
    "# for i, filename in enumerate(filenames):\n",
    "#     img_arr = imageio.imread(os.path.join(data_dir, filename))\n",
    "#     imt_t = torch.from_numpy(img_arr)\n",
    "#     img_t = img_t.permute(2, 0, 1)\n",
    "#     img_t = img_t[:3]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data_set\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # 缩放到224 * 224\n",
    "    # transforms.CenterCrop(256)   #中心剪裁后四周padding补充 (后续可以padding)\n",
    "     transforms.ToTensor()\n",
    "#     transforms.Normalize(mean=[.5, .5, .5], std=[.5, .5, .5])  # 均值为0 方差为1 的正态分布\n",
    "])\n",
    "\n",
    "# 0 -> mask  1-> nonmask  2 -> not a person\n",
    "train_dataset = ImageFolder('./sample_data/train_data', transform=transform)\n",
    "\n",
    "\n",
    "\n",
    "# for img, label in train_dataset:\n",
    "#     if (label == 0):\n",
    "#         print(img.shape)\n",
    "\n",
    "# # train 每四个为一组\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# # 显示图片\n",
    "# to_pil_image = transforms.ToPILImage()\n",
    "# for image, label in train_dataloader:\n",
    "#     img = to_pil_image(image[0])\n",
    "#     img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 256, 256, 17])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.4793, 0.4304, 0.3820])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate mean of imgs in each RGB channel\n",
    "imgs = torch.stack([img_t for img_t, _ in train_dataset], dim=3)\n",
    "print(imgs.shape)\n",
    "\n",
    "imgs.view(3, -1).mean(dim=1)  # mean of (number of imgs) in each channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2940, 0.2931, 0.2957])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs.view(3, -1).std(dim=1) # std of (number of imgs) in each channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset, normarlize it\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # 缩放到224 * 224\n",
    "    # transforms.CenterCrop(256)   #中心剪裁后四周padding补充 (后续可以padding)\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4793, 0.4304, 0.3820], std=[0.2940, 0.2931, 0.2957]) \n",
    "])\n",
    "\n",
    "# 0 -> mask  1-> nonmask  2 -> not a person\n",
    "train_dataset = ImageFolder('./sample_data/train_data', transform=transform)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 256, 256])\n",
      "0\n",
      "torch.Size([3, 256, 256])\n",
      "0\n",
      "torch.Size([3, 256, 256])\n",
      "0\n",
      "torch.Size([3, 256, 256])\n",
      "0\n",
      "torch.Size([3, 256, 256])\n",
      "0\n",
      "torch.Size([3, 256, 256])\n",
      "0\n",
      "torch.Size([3, 256, 256])\n",
      "1\n",
      "torch.Size([3, 256, 256])\n",
      "1\n",
      "torch.Size([3, 256, 256])\n",
      "1\n",
      "torch.Size([3, 256, 256])\n",
      "1\n",
      "torch.Size([3, 256, 256])\n",
      "1\n",
      "torch.Size([3, 256, 256])\n",
      "2\n",
      "torch.Size([3, 256, 256])\n",
      "2\n",
      "torch.Size([3, 256, 256])\n",
      "2\n",
      "torch.Size([3, 256, 256])\n",
      "2\n",
      "torch.Size([3, 256, 256])\n",
      "2\n",
      "torch.Size([3, 256, 256])\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for img, label in train_dataset:\n",
    "    print(img.shape)\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 256, 256])\n",
      "torch.Size([4])\n",
      "torch.Size([4, 3, 256, 256])\n",
      "torch.Size([4])\n",
      "torch.Size([4, 3, 256, 256])\n",
      "torch.Size([4])\n",
      "torch.Size([4, 3, 256, 256])\n",
      "torch.Size([4])\n",
      "torch.Size([1, 3, 256, 256])\n",
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for imgs, labels in train_dataloader:\n",
    "    print(imgs.shape)\n",
    "    print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn model\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_layer = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1), # in=3x256x256; out=32x256x256\n",
    "            nn.Tanh(),\n",
    "            nn.MaxPool2d(2), # out=64x128x128\n",
    "            nn.Conv2d(32, 16, kernel_size=3, padding=1), # in=32x128x128, out=16x128x128\n",
    "            nn.Tanh(),\n",
    "            nn.MaxPool2d(2), # out=16x64x64\n",
    "            nn.Conv2d(16, 8, kernel_size=3, padding=1), # in=16x64x64, out=8x64x64\n",
    "            nn.Tanh(),\n",
    "            nn.MaxPool2d(2) # out=8x32x32\n",
    "        )\n",
    "        \n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(8*32*32, 32*32),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(32*32, 1024),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(1024, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(128, 3)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # conv layer\n",
    "        x = self.conv_layer(x)\n",
    "        \n",
    "        # flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # fc layer\n",
    "        x = self.fc_layer(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define train_loop function\n",
    "def train_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_dataloader: # loop over batches in dataset\n",
    "            \n",
    "            outputs = model(imgs)  # feed a batch through our model\n",
    "            \n",
    "            loss = loss_fn(outputs, labels)  # computes the loss\n",
    "            \n",
    "            optimizer.zero_grad()  # getting rid of the gradients from the last round\n",
    "            \n",
    "            loss.backward()  # performs backward step, compute the gradients of all parameters\n",
    "            \n",
    "            optimizer.step()  # updates the model\n",
    "            \n",
    "            loss_train += loss.item() # sums of losses we saw over the epoch\n",
    "            \n",
    "        # print the average loss per batch, in epoch%10 == 0 \n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print('{} Epoch {}, Training loss {}'.format(\n",
    "                datetime.datetime.now(), epoch, loss_train/len(train_loader)\n",
    "            ))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-16 16:21:32.602623 Epoch 1, Training loss 1.1146415710449218\n",
      "2021-03-16 16:21:43.684629 Epoch 10, Training loss 0.2000491663813591\n",
      "2021-03-16 16:21:53.587626 Epoch 20, Training loss 0.017601943388581277\n",
      "2021-03-16 16:22:04.575631 Epoch 30, Training loss 0.010226001031696796\n",
      "2021-03-16 16:22:15.344626 Epoch 40, Training loss 0.005695029115304351\n",
      "2021-03-16 16:22:24.989631 Epoch 50, Training loss 0.0043560173362493515\n",
      "2021-03-16 16:22:34.597628 Epoch 60, Training loss 0.0031558499671518804\n",
      "2021-03-16 16:22:44.253632 Epoch 70, Training loss 0.0036084646359086036\n",
      "2021-03-16 16:22:53.933620 Epoch 80, Training loss 0.0021841708570718766\n",
      "2021-03-16 16:23:05.868623 Epoch 90, Training loss 0.002263228758238256\n",
      "2021-03-16 16:23:16.211620 Epoch 100, Training loss 0.0015468414523638785\n"
     ]
    }
   ],
   "source": [
    "# perform training, overfitting on small sample data with large amount of parameters\n",
    "model = CNN() # instantiates cnn model\n",
    "learning_rate = 0.02\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()  # use cross entropy loss function\n",
    "\n",
    "# call train_loop() function\n",
    "train_loop(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define validate function\n",
    "def validate(model, train_loader, test_loader):\n",
    "    model.eval()\n",
    "    # accuracy on training data and test data\n",
    "    for name, loader in [(\"train\", train_loader), (\"test\", test_loader)]:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad(): # do not want gradients here, as we will not want to update parameters\n",
    "            for imgs, labels in loader:\n",
    "                \n",
    "                outputs = model(imgs)  # feed input to models\n",
    "                \n",
    "                _, predicted = torch.max(outputs, dim=1)  # gives the index of the highest value as output\n",
    "                \n",
    "                total += labels.shape[0]  # counts the number of example, total is increased by the batch size\n",
    "                \n",
    "                # comparing the predicted class that had the maximum probability and the ground-truth labels,\n",
    "                # we first get a Boolean array. Taking the sum gives the number of items in the batch where \n",
    "                # the prediction and ground truth agree\n",
    "                correct += int((predicted == labels).sum()) \n",
    "        \n",
    "        print(\"Accuracy {}: {:.2f}\".format(name, correct / total))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 1.00\n",
      "Accuracy test: 0.40\n"
     ]
    }
   ],
   "source": [
    "# measuring accuracy\n",
    "test_dataset = ImageFolder('./sample_data/test_data', transform=transform)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "validate(model, train_dataloader, test_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
