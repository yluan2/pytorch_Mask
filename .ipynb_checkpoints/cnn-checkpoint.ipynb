{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read img files into a list\n",
    "# data_dir = './test_data/Mask/'\n",
    "# # os.listdir(data_dir)\n",
    "\n",
    "# filenames = [name for name in os.listdir(data_dir)\n",
    "#             if os.path.splitext(name)[-1] == '.jpg']\n",
    "\n",
    "# img_arr = imageio.imread(os.path.join(data_dir, filenames[0]))\n",
    "# img_arr.shape\n",
    "\n",
    "# batch_size = 5 # number of imgs in a batch\n",
    "# batch = torch.zeros(batch_size, 3, 256, 256)\n",
    "\n",
    "# for i, filename in enumerate(filenames):\n",
    "#     img_arr = imageio.imread(os.path.join(data_dir, filename))\n",
    "#     imt_t = torch.from_numpy(img_arr)\n",
    "#     img_t = img_t.permute(2, 0, 1)\n",
    "#     img_t = img_t[:3]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 256, 256, 17])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data_set\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # 缩放到224 * 224\n",
    "    # transforms.CenterCrop(256)   #中心剪裁后四周padding补充 (后续可以padding)\n",
    "     transforms.ToTensor()\n",
    "#     transforms.Normalize(mean=[.5, .5, .5], std=[.5, .5, .5])  # 均值为0 方差为1 的正态分布\n",
    "])\n",
    "\n",
    "# 0 -> mask  1-> nonmask  2 -> not a person\n",
    "train_dataset = ImageFolder('./test_data/', transform=transform)\n",
    "\n",
    "\n",
    "\n",
    "# for img, label in train_dataset:\n",
    "#     if (label == 0):\n",
    "#         print(img.shape)\n",
    "\n",
    "# # train 每四个为一组\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# # 显示图片\n",
    "# to_pil_image = transforms.ToPILImage()\n",
    "# for image, label in train_dataloader:\n",
    "#     img = to_pil_image(image[0])\n",
    "#     img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 256, 256, 17])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.4793, 0.4304, 0.3820])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate mean of imgs in each RGB channel\n",
    "imgs = torch.stack([img_t for img_t, _ in train_dataset], dim=3)\n",
    "print(imgs.shape)\n",
    "\n",
    "imgs.view(3, -1).mean(dim=1)  # mean of (number of imgs) in each channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2940, 0.2931, 0.2957])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs.view(3, -1).std(dim=1) # std of (number of imgs) in each channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset, normarlize it\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # 缩放到224 * 224\n",
    "    # transforms.CenterCrop(256)   #中心剪裁后四周padding补充 (后续可以padding)\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4793, 0.4304, 0.3820], std=[0.2940, 0.2931, 0.2957]) \n",
    "])\n",
    "\n",
    "# 0 -> mask  1-> nonmask  2 -> not a person\n",
    "train_dataset = ImageFolder('./test_data/', transform=transform)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 256, 256])\n",
      "0\n",
      "torch.Size([3, 256, 256])\n",
      "0\n",
      "torch.Size([3, 256, 256])\n",
      "0\n",
      "torch.Size([3, 256, 256])\n",
      "0\n",
      "torch.Size([3, 256, 256])\n",
      "0\n",
      "torch.Size([3, 256, 256])\n",
      "0\n",
      "torch.Size([3, 256, 256])\n",
      "1\n",
      "torch.Size([3, 256, 256])\n",
      "1\n",
      "torch.Size([3, 256, 256])\n",
      "1\n",
      "torch.Size([3, 256, 256])\n",
      "1\n",
      "torch.Size([3, 256, 256])\n",
      "1\n",
      "torch.Size([3, 256, 256])\n",
      "2\n",
      "torch.Size([3, 256, 256])\n",
      "2\n",
      "torch.Size([3, 256, 256])\n",
      "2\n",
      "torch.Size([3, 256, 256])\n",
      "2\n",
      "torch.Size([3, 256, 256])\n",
      "2\n",
      "torch.Size([3, 256, 256])\n",
      "2\n",
      "torch.Size([4, 3, 256, 256])\n",
      "torch.Size([4])\n",
      "torch.Size([4, 3, 256, 256])\n",
      "torch.Size([4])\n",
      "torch.Size([4, 3, 256, 256])\n",
      "torch.Size([4])\n",
      "torch.Size([4, 3, 256, 256])\n",
      "torch.Size([4])\n",
      "torch.Size([1, 3, 256, 256])\n",
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for img, label in train_dataset:\n",
    "    print(img.shape)\n",
    "    print(label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 256, 256])\n",
      "torch.Size([4])\n",
      "torch.Size([4, 3, 256, 256])\n",
      "torch.Size([4])\n",
      "torch.Size([4, 3, 256, 256])\n",
      "torch.Size([4])\n",
      "torch.Size([4, 3, 256, 256])\n",
      "torch.Size([4])\n",
      "torch.Size([1, 3, 256, 256])\n",
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for imgs, labels in train_dataloader:\n",
    "    print(imgs.shape)\n",
    "    print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
